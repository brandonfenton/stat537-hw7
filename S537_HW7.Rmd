---
title: 'Stat 537: Homework 7'
author: "Brandon Fenton and Kenny Flagg"
date: "Due Friday, March 11"
output: pdf_document
header-includes: \usepackage{float}
---


```{r setup, echo=F, message=F, warning=F}
require(pander)
require(clusterSim)
require(ca)
```


```{r p0_a, echo=T, warning=F, message=F, cache=T, fig.height=10, fig.width=10}
tc1<-read.csv("https://montana.box.com/shared/static/m5tv7r4ce7mw3w0vyqu0q7i1f8jflkkc.csv",header=T) #Data set without any modifications from https://knb.ecoinformatics.org/#view/doi:10.5063/F1DZ067F
tc1$responsef<-factor(tc1$response)
tc1_r<-tc1[,c(4:39,64:75)]

table(tc1_r$pack7)
## 
##         0 0.4916059 
##      2544         1
table(tc1_r$pack8)
## 
##          0 0.02702319 0.08039794 
##       2543          1          1
tc2<-tc1_r[,-c(43,44)]
cor1<-cor(tc2)

require(corrplot)
source("https://montana.box.com/shared/static/7ydjuwpraqpuovf7r1ovorsp828ckzs0.r")
corrplot_mg(cor1,order="hclust",tl.pos="lt")
```
1 _For the standardized (Q=46) variables in the data set tc2, perform a hierarchical cluster analysis of the observations using Ward's agglomeration method and Euclidean distance. Display a dendrogram and discuss the number of clusters you might think are reasonable based on this result._

```{r p1_a, echo=F, warning=F, message=F, cache=T, fig.height=8, fig.width=8}
# standardized?
tc2.std <- scale(tc2)
hc.wd2 <- hclust(dist(tc2.std), "ward.D2")


# looks ugly even with labels suppressed.
plot(hc.wd2, labels=F, xlab=NULL)
rect.hclust(hc.wd2, k=3)
```

Based on the dendrogram, we would cut the tree at a height of 150 and choose the three cluster solution. All splits lower down the tree occur relatively close together so there is little that additional splits produce clusters that are truly different.

2) _Assess the potential optimal number of clusters using the Calinski-Harabasz G1 measure (make a plot), considering 2 to 20 clusters. Discuss the results._
```{r p2_a, echo=F, comment=NA, fig.pos="H", fig.align="center", cache=T}
G1s <- numeric(20)
for(j in 1:20){G1s[j] <- index.G1(x=tc2.std,cl=cutree(hc.wd2,j))}

plot(1:20, G1s, type = "l", ylab = "G1 Index", xlab = "Number of clusters", xaxt="n")
axis(1, at=seq(2, 20, 2))
```

The G1 Index is maximized by the two cluster solution, which scores slightly higher than the three-cluster solution we chose based on the dendrogram. The index decreases monotonically as the number of clusters increases. The two cluster solution does the best job of maximizing between-cluster variability, but we cannot be sure that there really are clusters present because the G1 Index is undefined for one cluster.

3) _Now perform k-means cluster analysis on the standardized Q=46 data set and provide a plot of the ESS measure vs number of clusters for 2 to 20 clusters. Discuss the results and suggest a choice of clusters based on these results._

```{r p3_a, echo=F, message=F, warning=F, comment=NA, fig.pos="H", fig.align="center", cache=T, size="tiny"}

ESS <- numeric(20)
for(j in 1:20){ESS[j] <- kmeans(tc2.std, centers=j, iter.max=50, nstart=50)$tot.withinss}

plot(1:20, ESS, type = "l", ylab = "ESS", xlab = "Number of clusters", xaxt="n")
axis(1, at=seq(2, 20, 2))
```

ESS decreases each time the number of clusters is increased. The elbow of the plot occurs at 3 three clusters, suggesting that 3 clusters do a good job of describing the variation of the observations about the cluster means while additional clusters offer little improvement.

4) _Use G1 to pick the optimal number of clusters for kmeans and make a contingency table of the clusters identified from the optimal sized kmeans solution compared to the optimal selection from Ward's from question 2. Discuss the results. The optimal number of clusters need not agree for the two different algorithms._


```{r p4_a, echo=F, comment=NA, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}
G1.k <- numeric(20)
for(j in 1:20){G1.k[j] <- index.G1(x=tc2.std,cl=kmeans(tc2.std, centers=j, iter.max=50, nstart=50)$cluster)}

plot(1:20, G1.k, type = "l", ylab = "G1 Index", xlab = "Number of clusters", xaxt="n")
axis(1, at=seq(2, 20, 2))
```

For kmeans, G1 is once again maximized by the two cluster solution. The index descreases quickly as the number of clusters increases. If indeed there are multiple clusters present, G1 suggests there are two, but based on G1 alone we would be somewhat uncomfortable concluding that there are more than cluster.

```{r p4_b, echo=F, comment=NA, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}
kc2 <- kmeans(tc2.std, centers=2, nstart=50)
kc3 <- kmeans(tc2.std, centers=3, nstart=50)

hc2 <- cutree(hc.wd2, 2)
hc3 <- cutree(hc.wd2, 3)

contingency2 <- table("Kmeans" = kc2$cluster, "Ward's" = hc2)
colnames(contingency2) <- paste("Ward's Cluster", 1:2)
rownames(contingency2) <- paste("kmeans Cluster", 1:2)

pander(addmargins(contingency2, FUN = list("Total" = sum), quiet = T))
```

Several differences between the clustering methods are apparent in the contingency table. Ward's method placed almost two thirds of the observations in one cluster, while kmeans created clusters more balanced in size. The smaller cluster from Ward's method is essentially a subset of one of the kmeans clusters. Ward's method placed all of the remaining observations in a single cluster. Referring back to the dendrogram, the next split produced by Ward's method would partition this larger cluster into two roughly equally-sized clusters. This is very different from the kmeans approach, which tries to find the best solution given a number of clusters.

5) _Perform a correspondence analysis of the contingency table using the ca function from the ca package and discuss the results. Make sure to discuss the quality of the display and interpret all three aspects of the information provided in the plot in the context. Note: if the two solutions perfectly agree, select a different number of clusters for one of the two cluster analyses even if it isn't optimal._

The ca function from the ca package runs into errors when used on a table with two rows or two columns, so we instead consider the three cluster solutions since we chose these based on the dendrogram and ESS plot.

```{r p5_a, echo=F, comment=NA, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}
contingency3 <- table("Kmeans" = kc3$cluster, "Ward's" = hc3)
colnames(contingency3) <- paste("Ward Cluster", 1:3)
rownames(contingency3) <- paste("kmeans Cluster", 1:3)
pander(addmargins(contingency3, FUN = list("Total" = sum), quiet = T))

clust.ca <- ca(contingency3)

plot(clust.ca)
```


6) _In the original data set, the variable "response" is the presence (1) or absence (0) of white bark pine in each location. Make contingency tables of each of your cluster solutions with this variable, make the related side-by-side barcharts (try something like plot(tc1cluster))), and discuss whether some or all of your clusters seem to be related to the presence or absence of white bark pine._

```{r p6_a, warning=F,message=F, echo=F}
par(mfrow=c(1,2))
barplot(prop.table(table(tc1$response, hc3),2), beside=T,
        xlab="Hierarchical (Ward's)", ylab = "Proportion")
barplot(prop.table(table(tc1$response, kc3$cluster),2), beside=T,
        xlab="KMeans", ylab = "Proportion")

par(mfrow=c(1,1))
``` 
The first hierarchical cluster


```{r p6_b, warning=F, message=F, echo=F}

```  



7) _Plot your preferred cluster solution spatially using latitude and longitude (in other words plot lat~lon and add symbols and colors for the sites based on the cluster IDs). Discuss whether there appears to be spatial structure to the clusters. No tests, just a visual assessment based on the plot. You could talk about where each cluster tended to be located. To help with your visual assessment, you can permute the cluster labels using something like the following applied a vector of cluster labels called v1: require(mosaic) v1p<-shuffle(v1)_

```{r p7_a, echo=F, comment=NA, message=FALSE, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}

plot(tc1$lat~tc1$lon, type="n", xlab="Longitude", ylab="Latitude")

col.vec <- character(length(hc3))

alpha <- 0.2

cols <- numeric(0)
cols[1] <- rgb(0, 0, 1, alpha)
cols[2] <- rgb(0, 1, 1, alpha)
cols[3] <- rgb(1, 0, 1, alpha)

cols[4] <- rgb(0, 0, 1)
cols[5] <- rgb(0, 1, 1)
cols[6] <- rgb(1, 0, 1)

col.vec[hc3==1] <- cols[1]
col.vec[hc3==2] <- cols[2]
col.vec[hc3==3] <- cols[3]

# I want to plot medoids, but that can wait.
# medoids <- rbind(median(tc1$lat[hc]))
cluster1 <- cbind(tc1$lon[hc3==1], tc1$lat[hc3==1])
cluster2 <- cbind(tc1$lon[hc3==2], tc1$lat[hc3==2])
cluster3 <- cbind(tc1$lon[hc3==3], tc1$lat[hc3==3])

find.medoid <- function(cluster)
{
  med <- apply(cluster, 2, median)
  which.min(rowSums(abs(t(t(cluster) - apply(cluster, 2, median)))))
}

medoids <- matrix(nrow=3, ncol=2)
medoids[1,] <- cluster1[find.medoid(cluster1),]
medoids[2,] <- cluster2[find.medoid(cluster2),]
medoids[3,] <- cluster3[find.medoid(cluster3),]

centroids <- matrix(nrow=3, ncol=2)
centroids[1,] <- colMeans(cluster1)
centroids[2,] <- colMeans(cluster2)
centroids[3,] <- colMeans(cluster3)

segments(x0 = centroids[,1], x1 = medoids[,1],
         y0 = centroids[,2], y1 = medoids[,2],
         col=1)

points(x=tc1$lon, y=tc1$lat, pch=19, col=col.vec)
points(x=medoids[,1], y=medoids[,2], pch=21, bg=cols[4:6])

points(x=centroids[,1], y=centroids[,2], pch=20, col=2)



legend(x=-108.6, y=45, legend=1:3, pch=20,
       col=c(rgb(0, 0, 1, alpha),
             rgb(0, 1, 1, alpha),
             rgb(1, 0, 1, alpha)))
```

There appears to be a spatial structure to the clusters in terms of location relative to the area between latitudes 44 and 45, and between longitudes -111 and -110.  This relationship is very rough and there is a fair amount of overlap, but the points in cluster 2 mostly fall around the area centered around the aforementioned region, while the points in cluster 1 are further out than those in cluster 2 and the points in cluster 3 are almost exclusively found around the periphery.

```{r p7_b, echo=F, comment=NA, message=FALSE, fig.pos="H", fig.align="center", cache=T, size="footnotesize", fig.height=4, fig.width=10}

```

8) _Run the following code and explain the resulting plot. What is being displayed and what does it show?_ 


```{r p8_a, echo=F, comment=NA, message=FALSE, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}
d1<-dist(t(scale(tc2)))
d1_a<-sqrt(2*(1-(cor1))) 
plot(as.matrix(d1)~as.matrix(d1_a))
```

The y values in the above plot correspond to entries in the distance matrix between scaled variables in the __tc2__ dataframe.  The x values correspond to entries in the dissimilarity matrix for the variables in the __tc2__ dataframe.  There is an essentially perfect linear relationship between these two sets of values:

```{r p8_b, echo=F, comment=NA, warning=F, message=FALSE, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}
d.lm <- lm(as.vector(as.matrix(d1))~as.vector(as.matrix(d1_a)))
pander(summary(d.lm))
```

This implies that we are receiving the same information from both matrices.

# R Code Appendix:

Loading Data:
```{r a0, ref.label='p0_a', eval=F}
```

Problem 1:
```{r a1, ref.label='p1_a', eval=F}
```

Problem 2:
```{r a2, ref.label='p2_a', eval=F}
```

Problem 3:
```{r a3, ref.label='p3_a', eval=F}
```

Problem 4:
```{r a4, ref.label='p4_a', eval=F}
```
```{r b4, ref.label='p4_b', eval=F}
```

Problem 5:
```{r a5, ref.label='p5_a', eval=F}
```

Problem 6:
```{r a6, ref.label='p5_a', eval=F}
```

```{r a6b, ref.label='p5_b', eval=F}
```

Problem 7:
```{r a7, ref.label='p7_a', eval=F}
```
```{r b7, ref.label='p7_b', eval=F}
```

Problem 8:
```{r a8, ref.label='p8_a', eval=F}
```
```{r b8, ref.label='p8_b', eval=F}
```

<!--- ### About This Markdown File

  * File creation date: `r Sys.Date()`
  * `r R.version.string`
  * R version (short form): `r getRversion()`
  * Additional session information
  
```{r echo=FALSE}
sessionInfo()  # could use devtools::session_info() if you prefer that
```
-->



